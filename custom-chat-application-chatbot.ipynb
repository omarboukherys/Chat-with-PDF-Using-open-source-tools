{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7178472,"sourceType":"datasetVersion","datasetId":4148719},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#installation des packages nécessaires\n\n!pip install xformer --quiet\n!pip install chromadb --quiet\n!pip install langchain --quiet\n!pip install accelerate --quiet\n!pip install transformers --quiet\n!pip install bitsandbytes --quiet\n!pip install unstructured --quiet\n!pip install sentence-transformers --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.document_loaders import TextLoader  #for textfiles\nfrom langchain.text_splitter import CharacterTextSplitter #text splitter\nfrom langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\nfrom langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain import HuggingFaceHub\nfrom langchain.document_loaders import UnstructuredPDFLoader  #load pdf\nfrom langchain.indexes import VectorstoreIndexCreator #vectorize db index with chromadb\nfrom langchain.chains import RetrievalQA\nfrom langchain.document_loaders import UnstructuredURLLoader  #load urls into docoument-loader\nfrom langchain.prompts import PromptTemplate #to translate answers from english to french\nfrom langchain.llms import GPT4All ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textwrap import fill\nfrom IPython.display import Markdown, display\n\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    SystemMessagePromptTemplate,\n    )\n\nfrom langchain import PromptTemplate\nfrom langchain import HuggingFacePipeline\n\nfrom langchain.vectorstores import Chroma\nfrom langchain.schema import AIMessage, HumanMessage\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\nfrom langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install PyPDF2\nfrom PyPDF2 import PdfReader\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import FAISS","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **data preprocessing**","metadata":{}},{"cell_type":"markdown","source":"**Prepare data before vectorizing and feeding it to the Language Model**","metadata":{}},{"cell_type":"code","source":"data_reader= PdfReader(\"/kaggle/input/rapport-economique-data/rapport_economique.pdf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# printing number of pages in pdf file \nprint(len(data_reader.pages)) \n  \n# getting a specific page from the pdf file \npage = data_reader.pages[0] \n  \n# extracting text from page \ntext = page.extract_text() \nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content=\"\"\nfor i in range(0,len(data_reader.pages)):\n    page = data_reader.pages[i] \n    text = page.extract_text()\n    content=content+text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from the cintent we create multiple docs for making the retrieval more easy for the language model.\ntext_splitter = CharacterTextSplitter(\n    separator = \"\\n\",\n    chunk_size = 200,\n    chunk_overlap  = 20,\n    length_function = len,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = text_splitter.split_text(content)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Indexing**","metadata":{}},{"cell_type":"markdown","source":"**Represeting the created docs in a vectorial space using \"thenlper/gte-large\" embedding algorithm.**\n\n**!: You can use any embedding algorithm; updated and embedding algorithm are regularly published by the Data Science community and enthusiastes.**","metadata":{}},{"cell_type":"code","source":"embeddings = HuggingFaceEmbeddings(\n    model_name=\"thenlper/gte-large\",\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download the language model**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom time import time\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom transformers import pipeline\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.chains import LLMChain, SimpleSequentialChain\nfrom langchain import PromptTemplate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_id = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\ntime_1 = time()\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel_name = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n    )\nprint(f\"Tokenizer & pipeline: {round(time() - time_1)} sec.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_1 = time()\nquery_pipeline = pipeline(\n        \"text-generation\",\n        model=model_name,\n        tokenizer=tokenizer,\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.1,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        max_length=200,)\ntime_2 = time()\nprint(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm_mistral = HuggingFacePipeline(pipeline=query_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#install the vectorial data base.\n!pip install faiss-gpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#represent the docuemnt in vectorial space and store it into the vectorial database.\ndocsearch = FAISS.from_texts(texts, embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport requests\n#here remplace it by your own api_token\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"xxxxxxxx\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain import HuggingFaceHub\n\n\nllm_new_mistral=HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.1 ,\"max_length\":512})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Answer in french.\n\n{context}\n\nQuestion: {question}\nAnswer: French\n\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chain_type_kwargs = {\"prompt\": PROMPT}\nfrom langchain.chains import RetrievalQA\nchain = RetrievalQA.from_chain_type(llm=llm_new_mistral,\n                                    chain_type=\"stuff\",\n                                    retriever=docsearch.as_retriever(),\n                                    input_key=\"question\",\n                                    chain_type_kwargs=chain_type_kwargs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chain.run(\"quel est le taux de croissance prévu dans les économies emergentes en 2022?\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}